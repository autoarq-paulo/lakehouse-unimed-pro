services:
  postgres:
    image: postgres:16
    environment:
      POSTGRES_DB: ${PG_DB}
      POSTGRES_USER: ${PG_USER}
      POSTGRES_PASSWORD: ${PG_PASSWORD}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${PG_USER} -d ${PG_DB}"]
      interval: 10s
      timeout: 3s
      retries: 10
    volumes:
      - pgdata:/var/lib/postgresql/data
    networks: [lake]

  hive-metastore:
    image: apache/hive:3.1.3
    command: ["bash", "-c", "schematool -dbType postgres -initSchema && /opt/hive/bin/hive --service metastore"]
    environment:
      HIVE_METASTORE_DB_TYPE: postgres
      HIVE_METASTORE_URI: thrift://hive-metastore:9083
      HIVE_HOME: /opt/hive
      JDBC_URL: jdbc:postgresql://${PG_HOST}:${PG_PORT}/${PG_DB}
      JDBC_DRIVER: org.postgresql.Driver
      USER: ${PG_USER}
      PASSWORD: ${PG_PASSWORD}
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9083"]
      interval: 10s
      timeout: 3s
      retries: 10
    networks: [lake]

  minio:
    image: minio/minio:RELEASE.2025-02-28T00-00-00Z
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
    volumes:
      - minio:/data
    ports:
      - "9000:9000"
      - "9001:9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 12
    networks: [lake]

  trino:
    image: trinodb/trino:latest
    volumes:
      - ./deploy/trino/etc:/etc/trino
      - ./deploy/jmx/jmx_prometheus_javaagent-0.20.0.jar:/jmx.jar:ro
      - ./deploy/jmx/trino.yml:/trino-jmx.yml:ro
    environment:
      JVM_ARGS: "-javaagent:/jmx.jar=9404:/trino-jmx.yml"
    depends_on:
      - hive-metastore
      - minio
    ports:
      - "8080:8080"
    networks: [lake]

  spark:
    image: bitnami/spark:3.5
    environment:
      SPARK_MODE: master
      SPARK_DIST_CLASSPATH: /opt/spark/jars/*
      SPARK_DAEMON_JAVA_OPTS: "-javaagent:/jmx.jar=9308:/spark-jmx.yml"
    volumes:
      - ./jobs:/opt/jobs
      - ./deploy/spark/jars:/opt/spark/jars
      - ./deploy/jmx/jmx_prometheus_javaagent-0.20.0.jar:/jmx.jar:ro
      - ./deploy/jmx/spark.yml:/spark-jmx.yml:ro
    depends_on:
      - hive-metastore
      - minio
    ports:
      - "7077:7077"
      - "4040:4040"
    networks: [lake]

  airflow:
    image: apache/airflow:2.10.3
    environment:
      AIRFLOW_UID: ${AIRFLOW_UID}
      _PIP_ADDITIONAL_REQUIREMENTS: "apache-airflow-providers-apache-spark great-expectations"
    volumes:
      - ./deploy/airflow/dags:/opt/airflow/dags
      - ./jobs:/opt/jobs
      - ./dq:/opt/dq
    command: bash -c "airflow db init && airflow users create -u ${AIRFLOW_ADMIN_USER} -p ${AIRFLOW_ADMIN_PWD} -r Admin -e admin@example.com -f Admin -l User || true && airflow webserver & airflow scheduler"
    depends_on:
      - spark
    ports:
      - "8088:8080"
    networks: [lake]

  airflow-exporter:
    image: oboukili/airflow-prometheus-exporter:latest
    environment:
      - AIRFLOW_URI=http://airflow:8080
      - AIRFLOW_USERNAME=${AIRFLOW_ADMIN_USER}
      - AIRFLOW_PASSWORD=${AIRFLOW_ADMIN_PWD}
      - AIRFLOW_VERIFY_SSL=false
    depends_on:
      - airflow
    ports:
      - "9112:9112"
    networks: [lake]

  grafana:
    image: grafana/grafana:10.4.3
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_ADMIN_USER}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
    volumes:
      - ./deploy/grafana/provisioning:/etc/grafana/provisioning
      - ./deploy/grafana/dashboards:/var/lib/grafana/dashboards
    ports:
      - "3000:3000"
    networks: [lake]

networks:
  lake:

volumes:
  pgdata:
  minio:
